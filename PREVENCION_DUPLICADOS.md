# Sistema de Prevenci√≥n de Duplicados

## üõ°Ô∏è Resumen
Este sistema **garantiza cero duplicados** mediante una estrategia de doble capa:
1. **Validaci√≥n a nivel de aplicaci√≥n** - Verifica archivos ya procesados antes de insertar
2. **Constraints UNIQUE en base de datos** - PostgreSQL rechaza duplicados autom√°ticamente

---

## üìã Instrucciones de Uso

### Paso 1: Aplicar Constraints en la Base de Datos (SOLO UNA VEZ)

**IMPORTANTE:** Ejecuta este paso **ANTES** de usar `insertar_muestras.py` por primera vez.

```bash
# Opci√≥n A: Desde l√≠nea de comandos
psql -U postgres -d test_nps -f add_unique_constraints.sql

# Opci√≥n B: Desde pgAdmin o DBeaver
# Abre add_unique_constraints.sql y ejecuta el contenido
```

Este script:
- ‚úÖ Elimina duplicados existentes (si los hay)
- ‚úÖ Agrega constraint `UNIQUE(record_id, source_file)` en `banco_movil_clean`
- ‚úÖ Agrega constraint `UNIQUE(date_submitted, nps_score_bv, source_file)` en `banco_virtual_clean`
- ‚úÖ Crea √≠ndices en `source_file` para optimizar b√∫squedas

### Paso 2: Ejecutar el Pipeline Normalmente

```bash
python sample_extractor.py   # Extrae muestras
python sample_cleaner.py      # Limpia datos
python insertar_muestras.py   # Inserta en BD (ahora con protecci√≥n anti-duplicados)
```

---

## üîç C√≥mo Funciona

### Validaci√≥n de Archivos (Primera Barrera)

El script verifica `source_file` antes de procesar:

```python
# Si el archivo ya fue procesado:
if self.file_already_processed(file_name, 'banco_movil_clean'):
    logger.warning(f"‚äò OMITIDO - Archivo ya procesado: {file_name}")
    return True  # Se salta sin error
```

**Ventajas:**
- ‚ö° R√°pido - evita leer archivos grandes innecesariamente
- üìä Log claro de qu√© archivos se omiten
- üîÑ Permite re-ejecutar el script sin consecuencias

### Constraints de Base de Datos (Segunda Barrera)

Si por alguna raz√≥n la validaci√≥n falla, PostgreSQL rechaza duplicados:

```sql
-- Banco M√≥vil: record_id + source_file deben ser √∫nicos
ALTER TABLE banco_movil_clean
ADD CONSTRAINT unique_bm_record
UNIQUE (record_id, source_file);

-- Banco Virtual: fecha + score + source_file deben ser √∫nicos
ALTER TABLE banco_virtual_clean
ADD CONSTRAINT unique_bv_record
UNIQUE (date_submitted, nps_score_bv, source_file);
```

**Ventajas:**
- üõ°Ô∏è Protecci√≥n absoluta a nivel de base de datos
- üö® Error claro si se intenta insertar duplicados
- üí™ Funciona incluso si el script es modificado o bypassed

---

## üìä Ejemplo de Salida

### Primera Ejecuci√≥n (Archivos Nuevos)
```
============================================================
VERIFICANDO ARCHIVOS YA PROCESADOS
============================================================
No hay archivos procesados previamente (base de datos vac√≠a)

============================================================
Procesando 2 archivos BANCO M√ìVIL
============================================================

[1/2] agosto_bm_2025_limpio.xlsx
‚Üí Procesando Banco M√≥vil: agosto_bm_2025_limpio.xlsx
‚úì Banco M√≥vil insertado: 50000 registros de 50000

[2/2] septiembre_bm_2025_limpio.xlsx
‚Üí Procesando Banco M√≥vil: septiembre_bm_2025_limpio.xlsx
‚úì Banco M√≥vil insertado: 45000 registros de 45000

============================================================
RESUMEN DE INSERCI√ìN
============================================================
Banco M√≥vil:
  ‚úì Insertados: 95000 registros
  ‚äò Omitidos: 0 archivos (ya procesados)
Banco Virtual:
  ‚úì Insertados: 0 registros
  ‚äò Omitidos: 0 archivos (ya procesados)
Total insertado: 95000 registros
‚úì PIPELINE COMPLETADO - Sin duplicados garantizados
```

### Segunda Ejecuci√≥n (Archivos Ya Procesados)
```
============================================================
VERIFICANDO ARCHIVOS YA PROCESADOS
============================================================

Banco M√≥vil - 2 archivos en BD:
  ‚Ä¢ agosto_bm_2025_limpio.xlsx: 50000 registros (√∫ltima: 2025-10-09 14:23:11)
  ‚Ä¢ septiembre_bm_2025_limpio.xlsx: 45000 registros (√∫ltima: 2025-10-09 14:23:45)

============================================================
Procesando 2 archivos BANCO M√ìVIL
============================================================

[1/2] agosto_bm_2025_limpio.xlsx
‚äò OMITIDO - Archivo ya procesado: agosto_bm_2025_limpio.xlsx

[2/2] septiembre_bm_2025_limpio.xlsx
‚äò OMITIDO - Archivo ya procesado: septiembre_bm_2025_limpio.xlsx

============================================================
RESUMEN DE INSERCI√ìN
============================================================
Banco M√≥vil:
  ‚úì Insertados: 0 registros
  ‚äò Omitidos: 2 archivos (ya procesados)
Total insertado: 0 registros
‚úì PIPELINE COMPLETADO - Sin duplicados garantizados
```

---

## üö® Manejo de Errores

### Caso 1: Archivo Modificado con Mismo Nombre

Si modificas un archivo ya procesado y lo vuelves a limpiar, el script lo omitir√° por seguridad:

```
‚äò OMITIDO - Archivo ya procesado: agosto_bm_2025_limpio.xlsx
```

**Soluci√≥n:** Renombra el archivo (ej: `agosto_bm_2025_v2_limpio.xlsx`) antes de limpiarlo.

### Caso 2: Constraint Violation (Raro)

Si los datos duplicados logran pasar la validaci√≥n (escenario te√≥rico):

```
‚úó DUPLICADOS DETECTADOS - agosto_bm_2025_limpio.xlsx
  La base de datos rechaz√≥ registros duplicados: duplicate key value violates unique constraint "unique_bm_record"
  Verifica que el archivo no haya sido modificado y procesado nuevamente
```

**Soluci√≥n:** Este error indica una inconsistencia. Verifica:
1. ¬øEl archivo fue modificado manualmente?
2. ¬øHay m√∫ltiples versiones del mismo archivo?
3. ¬øSe ejecutaron scripts paralelos?

---

## üîß Comandos √ötiles

### Verificar Archivos Procesados
```sql
-- Ver todos los archivos en BD con su cantidad de registros
SELECT source_file, COUNT(*) as registros, MAX(inserted_at) as ultima_insercion
FROM banco_movil_clean
GROUP BY source_file
ORDER BY ultima_insercion DESC;

SELECT source_file, COUNT(*) as registros, MAX(inserted_at) as ultima_insercion
FROM banco_virtual_clean
GROUP BY source_file
ORDER BY ultima_insercion DESC;
```

### Buscar Duplicados (No Deber√≠a Haber)
```sql
-- Banco M√≥vil: busca duplicados por record_id + source_file
SELECT record_id, source_file, COUNT(*) as cantidad
FROM banco_movil_clean
GROUP BY record_id, source_file
HAVING COUNT(*) > 1;

-- Banco Virtual: busca duplicados por fecha + score + source_file
SELECT date_submitted, nps_score_bv, source_file, COUNT(*) as cantidad
FROM banco_virtual_clean
GROUP BY date_submitted, nps_score_bv, source_file
HAVING COUNT(*) > 1;
```

### Eliminar Archivo Espec√≠fico (Si Necesitas Reprocesar)
```sql
-- Elimina todos los registros de un archivo espec√≠fico
DELETE FROM banco_movil_clean WHERE source_file = 'agosto_bm_2025_limpio.xlsx';
DELETE FROM banco_virtual_clean WHERE source_file = 'agosto_bv_2025_limpio.xlsx';

-- Ahora puedes volver a ejecutar insertar_muestras.py
```

---

## ‚úÖ Garant√≠as del Sistema

Este sistema **GARANTIZA**:

1. ‚úÖ **Mismos datos no se insertan dos veces** - Validaci√≥n por `source_file`
2. ‚úÖ **Base de datos siempre consistente** - Constraints UNIQUE a nivel de PostgreSQL
3. ‚úÖ **Hist√≥ricos preservados** - `if_exists='append'` nunca borra datos
4. ‚úÖ **Logs claros y trazables** - Cada archivo tiene timestamp de inserci√≥n
5. ‚úÖ **Re-ejecuciones seguras** - Puedes correr el script cuantas veces quieras

---

## üìù Notas T√©cnicas

### Criterios de Unicidad

**Banco M√≥vil:**
- `record_id` (columna `id` del Excel) + `source_file`
- Rationale: Cada registro tiene un ID √∫nico dentro del archivo fuente

**Banco Virtual:**
- `date_submitted` + `nps_score_bv` + `source_file`
- Rationale: Combinaci√≥n de timestamp + score + fuente identifica registros √∫nicos

### Performance

- √çndices en `source_file` aceleran validaciones
- Query de verificaci√≥n: O(1) gracias al √≠ndice
- Inserci√≥n en batches de 1000 registros (par√°metro `chunksize`)

### Limitaciones

- Si cambias el nombre de un archivo ya procesado, se insertar√° como nuevo
- Si cambias el contenido pero mantienes el nombre, ser√° omitido
- Para reprocesar, debes eliminar manualmente los registros previos (ver comandos √∫tiles)

---

## üÜò Troubleshooting

### "No module named 'psycopg2'"
```bash
pip install psycopg2-binary
```

### "Cannot add constraint - duplicate keys exist"
Los datos ya tienen duplicados. El script `add_unique_constraints.sql` los elimina autom√°ticamente.

### "Table does not exist"
Ejecuta primero `insertar_muestras.py` una vez sin constraints para crear las tablas, luego aplica `add_unique_constraints.sql`.

---

**Autor:** Sistema NPS Pipeline
**√öltima actualizaci√≥n:** 2025-10-09
