# Pipeline de Procesamiento de Datos NPS

Sistema automatizado para extraer, limpiar e insertar datos de encuestas NPS (Net Promoter Score) desde archivos Excel a PostgreSQL.

## üìã Tabla de Contenidos

- [Requisitos](#requisitos)
- [Configuraci√≥n Inicial](#configuraci√≥n-inicial)
- [Estructura del Proyecto](#estructura-del-proyecto)
- [Orden de Ejecuci√≥n](#orden-de-ejecuci√≥n)
- [Scripts Principales](#scripts-principales)
- [Scripts Auxiliares](#scripts-auxiliares)
- [Soluci√≥n de Problemas](#soluci√≥n-de-problemas)

---

## üîß Requisitos

### Software necesario:
- **Python 3.8+**
- **PostgreSQL 12+**

### Librer√≠as Python:
```bash
pip install pandas sqlalchemy psycopg2-binary openpyxl
```

---

## ‚öôÔ∏è Configuraci√≥n Inicial

### 1. Crear base de datos en PostgreSQL

```sql
CREATE DATABASE nps_analytics;
```

**‚ö†Ô∏è Importante:** La base de datos se llama `nps_analytics` (nombre de producci√≥n, no usar "test" o "sample")

### 2. Actualizar credenciales en los scripts

Edita la variable `DB_CONFIG` en los siguientes archivos:

- `test_connection.py`
- `insertar_muestras.py`
- `setup_constraints.py`
- `inspect_database.py`
- `cleanup_database.py`

```python
DB_CONFIG = {
    'host': 'localhost',
    'port': '5432',
    'database': 'nps_analytics',  # ‚¨ÖÔ∏è Cambiar aqu√≠
    'user': 'postgres',
    'password': 'TU_CONTRASE√ëA'   # ‚¨ÖÔ∏è Cambiar aqu√≠
}
```

### 3. Crear estructura de carpetas

El sistema espera la siguiente estructura (se crea autom√°ticamente al ejecutar scripts):

```
datos/
‚îú‚îÄ‚îÄ data-cruda/          # Archivos Excel originales (input)
‚îú‚îÄ‚îÄ datos_raw/           # Datos extra√≠dos en CSV (generado)
‚îú‚îÄ‚îÄ datos_clean/         # Datos limpios listos para inserci√≥n (generado)
‚îú‚îÄ‚îÄ test_connection.py
‚îú‚îÄ‚îÄ data_extractor.py
‚îú‚îÄ‚îÄ data_cleaner.py
‚îú‚îÄ‚îÄ insertar_muestras.py
‚îú‚îÄ‚îÄ setup_constraints.py
‚îú‚îÄ‚îÄ inspect_database.py
‚îî‚îÄ‚îÄ cleanup_database.py
```

---

## üìÇ Estructura del Proyecto

### Carpetas de datos:

| Carpeta | Prop√≥sito | Contenido |
|---------|-----------|-----------|
| `data-cruda/` | **Input** - Archivos originales | Archivos Excel mensuales (`agosto_bm_2025.xlsx`, `agosto_bv_2025.xlsx`) |
| `datos_raw/` | **Intermediario** - Datos extra√≠dos | CSV con datos sin procesar (`agosto_bm_2025_raw.csv`) |
| `datos_clean/` | **Intermediario** - Datos limpios | CSV listos para inserci√≥n (`agosto_bm_2025_clean.csv`) |

### Tablas de PostgreSQL:

| Tabla | Registros | Tama√±o | Descripci√≥n |
|-------|-----------|--------|-------------|
| `banco_movil_clean` | 1.2M+ | 704 MB | Datos de encuestas Banco M√≥vil (BM) |
| `banco_virtual_clean` | 5.7K+ | 2 MB | Datos de encuestas Banco Virtual (BV) |

---

## üöÄ Orden de Ejecuci√≥n

### Pipeline Completo (Primera Vez)

Sigue este orden exacto para procesar datos nuevos:

#### **Paso 1: Validar Conexi√≥n a Base de Datos**

```bash
python test_connection.py
```

**¬øQu√© hace?**
- Verifica conectividad a PostgreSQL
- Prueba inserci√≥n de datos de prueba
- Valida encoding UTF-8 para caracteres especiales

**Resultado esperado:**
```
‚úÖ Conexi√≥n exitosa a PostgreSQL
PostgreSQL version: 16.x
‚úÖ Datos de prueba insertados correctamente
```

---

#### **Paso 2: Extraer Datos de Excel**

```bash
python data_extractor.py
```

**¬øQu√© hace?**
- Lee archivos Excel desde `data-cruda/`
- Extrae hasta 300,000 registros por archivo
- Genera archivos CSV en `datos_raw/`

**Input:** `data-cruda/agosto_bm_2025.xlsx`, `data-cruda/agosto_bv_2025.xlsx`

**Output:** `datos_raw/agosto_bm_2025_raw.csv`, `datos_raw/agosto_bv_2025_raw.csv`

**Resultado esperado:**
```
‚úÖ Extra√≠dos 50,000 registros de agosto_bm_2025.xlsx ‚Üí datos_raw/agosto_bm_2025_raw.csv
‚úÖ Extra√≠dos 200 registros de agosto_bv_2025.xlsx ‚Üí datos_raw/agosto_bv_2025_raw.csv
```

---

#### **Paso 3: Limpiar y Transformar Datos**

```bash
python data_cleaner.py
```

**¬øQu√© hace?**
- Lee archivos CSV desde `datos_raw/`
- Corrige encoding UTF-8 malformado (√É¬≥‚Üí√≥, √É¬°‚Üí√°)
- Expande JSON de respuestas (solo BM)
- Calcula categor√≠a NPS (Detractor/Neutral/Promotor)
- Genera archivos CSV limpios en `datos_clean/`

**Input:** `datos_raw/agosto_bm_2025_raw.csv`

**Output:** `datos_clean/agosto_bm_2025_clean.csv`

**Resultado esperado:**
```
‚úÖ Limpiados 50,000 registros de agosto_bm_2025_raw.csv ‚Üí datos_clean/agosto_bm_2025_clean.csv
‚úÖ Limpiados 200 registros de agosto_bv_2025_raw.csv ‚Üí datos_clean/agosto_bv_2025_clean.csv
```

---

#### **Paso 4: Configurar Restricciones UNIQUE (Solo primera vez)**

```bash
python setup_constraints.py
```

**¬øQu√© hace?**
- Crea restricciones UNIQUE en tablas de producci√≥n
- Previene duplicados a nivel de base de datos
- **Ejecutar solo una vez** al configurar la base de datos

**Restricciones creadas:**
- `banco_movil_clean`: UNIQUE(record_id, source_file)
- `banco_virtual_clean`: UNIQUE(date_submitted, nps_score_bv, source_file)

**Resultado esperado:**
```
‚úÖ Constraint creado: banco_movil_clean UNIQUE(record_id, source_file)
‚úÖ Constraint creado: banco_virtual_clean UNIQUE(date_submitted, nps_score_bv, source_file)
```

---

#### **Paso 5: Insertar Datos en PostgreSQL**

```bash
python insertar_muestras.py
```

**¬øQu√© hace?**
- Lee archivos CSV desde `datos_clean/`
- Verifica si el archivo ya fue insertado (prevenci√≥n de duplicados)
- Inserta datos en `banco_movil_clean` o `banco_virtual_clean`
- Crea √≠ndices para consultas r√°pidas

**Input:** `datos_clean/agosto_bm_2025_clean.csv`

**Output:** Registros en tablas PostgreSQL

**Prevenci√≥n de duplicados:**
- ‚úÖ **Aplicaci√≥n:** Verifica `source_file` antes de insertar
- ‚úÖ **Base de datos:** UNIQUE constraints rechazan duplicados

**Resultado esperado:**
```
‚úÖ Insertados 50,000 registros en banco_movil_clean desde agosto_bm_2025_clean.csv
‚úÖ Insertados 200 registros en banco_virtual_clean desde agosto_bv_2025_clean.csv
‚ö†Ô∏è  Archivo agosto_bm_2025_clean.csv ya fue insertado previamente (omitido)
```

---

### Pipeline Incremental (Meses Posteriores)

Para procesar nuevos meses de datos:

```bash
# 1. Colocar nuevos archivos Excel en data-cruda/
#    Ejemplo: septiembre_bm_2025.xlsx, septiembre_bv_2025.xlsx

# 2. Ejecutar pipeline (sin setup_constraints.py)
python data_extractor.py
python data_cleaner.py
python insertar_muestras.py
```

**‚ö†Ô∏è Nota:** `setup_constraints.py` solo se ejecuta una vez. Los siguientes meses solo requieren los pasos 2, 3 y 5.

---

## üìö Scripts Principales

### 1. test_connection.py
**Prop√≥sito:** Validar configuraci√≥n de base de datos

**Usa este script cuando:**
- Configuras el sistema por primera vez
- Cambias credenciales de base de datos
- Detectas problemas de conexi√≥n

**Ejecuci√≥n:**
```bash
python test_connection.py
```

---

### 2. data_extractor.py
**Prop√≥sito:** Extraer datos desde archivos Excel a CSV

**Configuraci√≥n:**
- L√≠nea 12: `max_records = 300000` (cambiar para extraer menos registros en pruebas)

**Detecta archivos por patr√≥n:**
- `*_bm_*.xlsx` ‚Üí Banco M√≥vil
- `*_bv_*.xlsx` ‚Üí Banco Virtual

**Ejecuci√≥n:**
```bash
python data_extractor.py
```

---

### 3. data_cleaner.py
**Prop√≥sito:** Limpiar y transformar datos

**Operaciones clave:**
- ‚úÖ Correcci√≥n de encoding UTF-8
- ‚úÖ Expansi√≥n de JSON `answers` (BM)
- ‚úÖ C√°lculo de NPS score
- ‚úÖ Categorizaci√≥n (Detractor/Neutral/Promotor)
- ‚úÖ Normalizaci√≥n de fechas

**Ejecuci√≥n:**
```bash
python data_cleaner.py
```

---

### 4. insertar_muestras.py
**Prop√≥sito:** Insertar datos limpios en PostgreSQL

**Caracter√≠sticas:**
- ‚úÖ Prevenci√≥n de duplicados (aplicaci√≥n + base de datos)
- ‚úÖ Inserci√≥n en lotes (1000 registros por batch)
- ‚úÖ Creaci√≥n autom√°tica de √≠ndices
- ‚úÖ Logging detallado

**Ejecuci√≥n:**
```bash
python insertar_muestras.py
```

---

## üõ†Ô∏è Scripts Auxiliares

### setup_constraints.py
**Prop√≥sito:** Configurar restricciones UNIQUE (una sola vez)

**Cu√°ndo ejecutar:**
- ‚úÖ Primera vez que configuras la base de datos
- ‚ùå NO ejecutar en cada inserci√≥n de datos

**Ejecuci√≥n:**
```bash
python setup_constraints.py
```

---

### inspect_database.py
**Prop√≥sito:** Generar documentaci√≥n de estructura de base de datos

**Output:** Archivo `database_structure.txt` con:
- Lista de tablas
- Columnas y tipos de datos
- Constraints e √≠ndices
- Registros de ejemplo
- Tama√±o de tablas

**Ejecuci√≥n:**
```bash
python inspect_database.py
```

---

### cleanup_database.py
**Prop√≥sito:** Eliminar tablas obsoletas de prueba

**Modos:**
```bash
# Modo dry-run (solo muestra qu√© se eliminar√≠a)
python cleanup_database.py

# Modo ejecuci√≥n (elimina tablas confirmadas)
python cleanup_database.py --execute
```

**‚ö†Ô∏è Tablas protegidas (NUNCA se eliminan):**
- `banco_movil_clean`
- `banco_virtual_clean`

---

## üêõ Soluci√≥n de Problemas

### Error: "No se puede conectar a PostgreSQL"

**Soluci√≥n:**
1. Verifica que PostgreSQL est√© corriendo:
   ```bash
   # Windows
   sc query postgresql-x64-16

   # Linux/Mac
   sudo systemctl status postgresql
   ```
2. Revisa credenciales en `DB_CONFIG`
3. Ejecuta `python test_connection.py`

---

### Error: "IntegrityError: duplicate key value violates unique constraint"

**Causa:** Intentando insertar un archivo que ya existe en la base de datos

**Soluci√≥n:**
- ‚úÖ Este es el comportamiento esperado (prevenci√≥n de duplicados)
- El archivo ser√° omitido autom√°ticamente
- No requiere acci√≥n

---

### Error: "UnicodeDecodeError" al leer CSV

**Causa:** Archivo con encoding incorrecto

**Soluci√≥n:**
1. Verifica que los archivos CSV se guardaron con UTF-8
2. Ejecuta `data_cleaner.py` nuevamente
3. Si persiste, abre el CSV en un editor y guarda con UTF-8

---

### Warning: "JSON inv√°lido encontrado"

**Causa:** Campo `answers` con formato JSON corrupto (com√∫n en datos BM)

**Soluci√≥n:**
- ‚úÖ El script maneja esto autom√°ticamente
- Convierte JSON inv√°lido a `[]` (array vac√≠o)
- No detiene el proceso

---

### Pregunta: "¬øC√≥mo proceso solo un archivo de prueba?"

**Soluci√≥n:**
1. Edita `data_extractor.py` l√≠nea 12:
   ```python
   max_records = 100  # Cambiar de 300000 a 100
   ```
2. Ejecuta pipeline normalmente
3. Restaura `max_records = 300000` para producci√≥n

---

### Pregunta: "¬øC√≥mo verifico que los datos se insertaron correctamente?"

**Soluci√≥n:**
```bash
# Generar reporte de base de datos
python inspect_database.py

# Revisar archivo generado
cat database_structure.txt
```

O consulta directamente en PostgreSQL:
```sql
-- Contar registros
SELECT COUNT(*) FROM banco_movil_clean;
SELECT COUNT(*) FROM banco_virtual_clean;

-- Ver √∫ltimos 10 registros
SELECT * FROM banco_movil_clean ORDER BY inserted_at DESC LIMIT 10;
```

---

## üìä Archivos de Log

El sistema genera logs detallados:

| Archivo | Contenido |
|---------|-----------|
| `data_cleaning.log` | Operaciones de limpieza y transformaci√≥n |
| `insercion_datos.log` | Inserciones en base de datos, duplicados detectados |

**Revisar logs:**
```bash
# √öltimas 50 l√≠neas de log de limpieza
tail -n 50 data_cleaning.log

# √öltimas 50 l√≠neas de log de inserci√≥n
tail -n 50 insercion_datos.log
```

---

## üìà Flujo Visual del Pipeline

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   data-cruda/       ‚îÇ
‚îÇ  agosto_bm_2025.xlsx‚îÇ ‚îÄ‚îÄ‚îê
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
                          ‚îÇ python data_extractor.py
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ   data-cruda/       ‚îÇ   ‚îÇ
‚îÇ  agosto_bv_2025.xlsx‚îÇ ‚îÄ‚îÄ‚î§
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
                          ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   datos_raw/        ‚îÇ
                    ‚îÇ agosto_bm_2025_raw  ‚îÇ
                    ‚îÇ agosto_bv_2025_raw  ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚îÇ
                          ‚îÇ python data_cleaner.py
                          ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   datos_clean/      ‚îÇ
                    ‚îÇ agosto_bm_2025_clean‚îÇ
                    ‚îÇ agosto_bv_2025_clean‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚îÇ
                          ‚îÇ python insertar_muestras.py
                          ‚ñº
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   PostgreSQL        ‚îÇ
                    ‚îÇ nps_analytics       ‚îÇ
                    ‚îÇ                     ‚îÇ
                    ‚îÇ banco_movil_clean   ‚îÇ (1.2M records)
                    ‚îÇ banco_virtual_clean ‚îÇ (5.7K records)
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## ‚úÖ Checklist de Primera Ejecuci√≥n

- [ ] PostgreSQL instalado y corriendo
- [ ] Base de datos `nps_analytics` creada
- [ ] Librer√≠as Python instaladas (`pandas`, `sqlalchemy`, `psycopg2-binary`, `openpyxl`)
- [ ] Credenciales actualizadas en todos los scripts (`DB_CONFIG`)
- [ ] Archivos Excel colocados en `data-cruda/`
- [ ] Ejecutado: `python test_connection.py` ‚úÖ
- [ ] Ejecutado: `python data_extractor.py` ‚úÖ
- [ ] Ejecutado: `python data_cleaner.py` ‚úÖ
- [ ] Ejecutado: `python setup_constraints.py` ‚úÖ (solo primera vez)
- [ ] Ejecutado: `python insertar_muestras.py` ‚úÖ
- [ ] Verificado: `python inspect_database.py` ‚úÖ

---

## üìû Soporte

Para m√°s detalles t√©cnicos, consulta:
- [CLAUDE.md](CLAUDE.md) - Documentaci√≥n completa del proyecto
- [PREVENCION_DUPLICADOS.md](PREVENCION_DUPLICADOS.md) - Sistema de prevenci√≥n de duplicados
- `database_structure.txt` - Esquema de base de datos (generado por `inspect_database.py`)

---

**√öltima actualizaci√≥n:** 2025-10-09
