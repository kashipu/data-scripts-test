#!/usr/bin/env python3
"""
======================================================================================
SCRIPT: 1_extractor.py
======================================================================================
PROP√ìSITO:
    Extrae datos de encuestas NPS desde archivos Excel originales y los convierte
    a formato Excel para su posterior limpieza e inserci√≥n en PostgreSQL.

QU√â HACE:
    1. Escanea el directorio 'datos/raw/' buscando archivos Excel organizados por mes
    2. Verifica si un archivo ya fue procesado (usando archivo de tracking)
    3. Identifica autom√°ticamente archivos de Banco M√≥vil (BM) y Banco Virtual (BV)
    4. Lee los archivos Excel completos o limitados seg√∫n configuraci√≥n
    5. Guarda los datos extra√≠dos en formato Excel en el directorio 'datos/procesados/'
    6. Genera un log individual (.txt) por cada archivo procesado
    7. Actualiza archivo de tracking para evitar reprocesamiento

ESTRUCTURA DE ENTRADA ESPERADA:
    datos/raw/
    ‚îú‚îÄ‚îÄ Agosto/
    ‚îÇ   ‚îú‚îÄ‚îÄ Agosto_BM_2025.xlsx  # Archivo Banco M√≥vil
    ‚îÇ   ‚îî‚îÄ‚îÄ Agosto_BV_2025.xlsx  # Archivo Banco Virtual
    ‚îú‚îÄ‚îÄ Septiembre/
    ‚îÇ   ‚îú‚îÄ‚îÄ Septiembre_BM_2025.xlsx
    ‚îÇ   ‚îî‚îÄ‚îÄ Septiembre_BV_2025.xlsx

ARCHIVOS DE SALIDA:
    datos/procesados/
    ‚îú‚îÄ‚îÄ Agosto_BM_2025_extracted_50000.xlsx
    ‚îú‚îÄ‚îÄ Agosto_BM_2025_extracted_50000.txt  ‚Üê LOG individual
    ‚îú‚îÄ‚îÄ Agosto_BV_2025_extracted_200.xlsx
    ‚îú‚îÄ‚îÄ Agosto_BV_2025_extracted_200.txt    ‚Üê LOG individual
    ‚îî‚îÄ‚îÄ .processed_files.txt                ‚Üê TRACKING de archivos procesados

SISTEMA DE TRACKING:
    - Archivo '.processed_files.txt' registra todos los archivos ya procesados
    - Evita reprocesar archivos que ya fueron extra√≠dos exitosamente
    - Se puede forzar reprocesamiento con flag --force

OPCIONES DE USO:
    python 1_extractor.py                              # Procesa todos los archivos nuevos
    python 1_extractor.py --full                       # Procesa archivos completos (sin l√≠mite)
    python 1_extractor.py --limit 5000                 # Limita a 5000 registros por archivo
    python 1_extractor.py --file "Agosto_BM_2025.xlsx" # Procesa solo un archivo espec√≠fico
    python 1_extractor.py --force                      # Fuerza reprocesamiento de todos

CU√ÅNDO EJECUTAR:
    - Primera vez al configurar el pipeline
    - Cada mes cuando lleguen nuevos archivos Excel con encuestas NPS
    - Despu√©s de agregar nuevos datos a 'datos/raw/'

RESULTADO ESPERADO:
    ‚úÖ Extra√≠dos 50,000 registros de Agosto_BM_2025.xlsx ‚Üí datos/procesados/Agosto_BM_2025_extracted_50000.xlsx
    ‚úÖ Log generado: datos/procesados/Agosto_BM_2025_extracted_50000.txt
    ‚è≠Ô∏è  Omitiendo Septiembre_BM_2025.xlsx (ya procesado anteriormente)

SIGUIENTE PASO:
    Ejecutar: python 2_limpieza.py
======================================================================================
"""

import pandas as pd
import os
import argparse
import logging
from pathlib import Path
from datetime import datetime
import hashlib

# ======================================================================================
# CONFIGURACI√ìN
# ======================================================================================

# Directorio donde est√°n los archivos Excel originales
DIRECTORIO_ENTRADA = "datos/raw"

# Directorio donde se guardar√°n los datos extra√≠dos
DIRECTORIO_SALIDA = "datos/procesados"

# Archivo de tracking para evitar reprocesamiento
ARCHIVO_TRACKING = os.path.join(DIRECTORIO_SALIDA, ".processed_files.txt")

# Configuraci√≥n de logging global
LOG_GENERAL = "extraccion_datos.log"

# ======================================================================================
# CONFIGURACI√ìN DE LOGGING
# ======================================================================================

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_GENERAL, encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ======================================================================================
# SISTEMA DE TRACKING DE ARCHIVOS PROCESADOS
# ======================================================================================

def obtener_hash_archivo(ruta_archivo):
    """
    Genera un hash √∫nico del archivo para verificar si cambi√≥

    Args:
        ruta_archivo (str): Ruta al archivo

    Returns:
        str: Hash MD5 del archivo
    """
    hash_md5 = hashlib.md5()
    try:
        with open(ruta_archivo, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    except Exception as e:
        logger.warning(f"No se pudo calcular hash de {ruta_archivo}: {e}")
        return None


def cargar_archivos_procesados():
    """
    Carga el registro de archivos ya procesados

    Returns:
        dict: Diccionario con {ruta_archivo: hash}
    """
    if not os.path.exists(ARCHIVO_TRACKING):
        return {}

    archivos_procesados = {}
    try:
        with open(ARCHIVO_TRACKING, 'r', encoding='utf-8') as f:
            for linea in f:
                linea = linea.strip()
                if linea and '|' in linea:
                    ruta, hash_archivo = linea.split('|', 1)
                    archivos_procesados[ruta] = hash_archivo
    except Exception as e:
        logger.error(f"Error leyendo archivo de tracking: {e}")

    return archivos_procesados


def registrar_archivo_procesado(ruta_archivo, hash_archivo):
    """
    Registra un archivo como procesado en el tracking

    Args:
        ruta_archivo (str): Ruta del archivo procesado
        hash_archivo (str): Hash del archivo
    """
    try:
        # Crear directorio si no existe
        Path(DIRECTORIO_SALIDA).mkdir(exist_ok=True)

        with open(ARCHIVO_TRACKING, 'a', encoding='utf-8') as f:
            f.write(f"{ruta_archivo}|{hash_archivo}\n")

        logger.info(f"üìù Archivo registrado en tracking: {os.path.basename(ruta_archivo)}")
    except Exception as e:
        logger.error(f"Error registrando archivo en tracking: {e}")


def archivo_ya_procesado(ruta_archivo, archivos_procesados):
    """
    Verifica si un archivo ya fue procesado

    Args:
        ruta_archivo (str): Ruta del archivo a verificar
        archivos_procesados (dict): Diccionario de archivos procesados

    Returns:
        bool: True si ya fue procesado y no cambi√≥, False en caso contrario
    """
    if ruta_archivo not in archivos_procesados:
        return False

    # Verificar si el archivo cambi√≥ (comparando hash)
    hash_actual = obtener_hash_archivo(ruta_archivo)
    hash_registrado = archivos_procesados.get(ruta_archivo)

    if hash_actual == hash_registrado:
        return True
    else:
        logger.info(f"‚ö†Ô∏è  Archivo modificado detectado: {os.path.basename(ruta_archivo)}")
        return False


# ======================================================================================
# FUNCIONES DE EXTRACCI√ìN
# ======================================================================================

def generar_log_individual(ruta_archivo_salida, info_extraccion):
    """
    Genera un archivo .txt con informaci√≥n detallada de la extracci√≥n

    Args:
        ruta_archivo_salida (str): Ruta del archivo Excel de salida
        info_extraccion (dict): Diccionario con informaci√≥n de la extracci√≥n
    """
    ruta_log = ruta_archivo_salida.replace('.xlsx', '.txt')

    try:
        with open(ruta_log, 'w', encoding='utf-8') as f:
            f.write("="*80 + "\n")
            f.write("LOG DE EXTRACCI√ìN - 1_extractor.py\n")
            f.write("="*80 + "\n\n")

            f.write(f"Fecha de extracci√≥n: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Archivo original: {info_extraccion.get('archivo_original', 'N/A')}\n")
            f.write(f"Archivo de salida: {os.path.basename(ruta_archivo_salida)}\n")
            f.write(f"Tipo de archivo: {info_extraccion.get('tipo', 'N/A')}\n\n")

            f.write(f"ESTAD√çSTICAS:\n")
            f.write(f"  - Total registros en archivo original: {info_extraccion.get('total_original', 0):,}\n")
            f.write(f"  - Registros extra√≠dos: {info_extraccion.get('registros_extraidos', 0):,}\n")
            f.write(f"  - Total columnas: {info_extraccion.get('total_columnas', 0)}\n\n")

            if info_extraccion.get('columnas_importantes'):
                f.write(f"COLUMNAS CLAVE DETECTADAS:\n")
                for col in info_extraccion['columnas_importantes']:
                    f.write(f"  ‚Ä¢ {col}\n")
                f.write("\n")

            if info_extraccion.get('advertencias'):
                f.write(f"ADVERTENCIAS:\n")
                for adv in info_extraccion['advertencias']:
                    f.write(f"  ‚ö†Ô∏è  {adv}\n")
                f.write("\n")

            # Agregar resumen de validaci√≥n
            if 'validacion' in info_extraccion:
                val = info_extraccion['validacion']
                f.write(f"VALIDACI√ìN DE CALIDAD:\n")
                f.write(f"  - Tasa de calidad: {val.get('tasa_calidad', 0):.1f}%\n")
                f.write(f"  - Estado: {val.get('estado', 'N/A')}\n")
                f.write(f"  - Cr√≠ticos: {len(val.get('criticos', []))}\n")
                f.write(f"  - Advertencias: {len(val.get('advertencias', []))}\n")
                f.write(f"  - Duplicados reales: {val.get('duplicados_reales', 0)}\n")
                f.write(f"  üìã Ver detalles completos en archivo .validation\n\n")

            f.write(f"ESTADO: {'‚úÖ Exitoso' if info_extraccion.get('exitoso') else '‚ùå Con errores'}\n")
            f.write("="*80 + "\n")

        logger.info(f"üìÑ Log individual generado: {os.path.basename(ruta_log)}")

    except Exception as e:
        logger.error(f"Error generando log individual: {e}")


def validar_datos_detallado(df, ruta_archivo_salida, tipo_archivo):
    """
    Valida datos extra√≠dos y genera reporte detallado categorizando errores por severidad

    CATEGOR√çAS:
    - CR√çTICOS: Bloquean el procesamiento
    - ADVERTENCIAS: Revisar pero no bloquean
    - INFORMATIVOS: Se corrigen autom√°ticamente en 2_limpieza.py

    Args:
        df (DataFrame): DataFrame con datos extra√≠dos
        ruta_archivo_salida (str): Ruta del archivo de salida
        tipo_archivo (str): 'BM' o 'BV'

    Returns:
        dict: Diccionario con resultados de validaci√≥n categorizados
    """
    validacion = {
        'total_filas': len(df),
        'filas_validas': 0,
        'filas_con_criticos': 0,
        'filas_con_advertencias': 0,
        'filas_con_informativos': 0,

        # Errores por severidad
        'criticos': [],
        'advertencias': [],
        'informativos': [],

        # An√°lisis de duplicados
        'duplicados_por_id': 0,
        'duplicados_reales': 0,

        # Otras m√©tricas
        'columnas_criticas_faltantes': [],
        'valores_nulos_por_columna': {},
        'errores_encoding': 0,
        'tiene_encoding_corrupto': False
    }

    # =========================================================================
    # 1. VERIFICAR COLUMNAS CR√çTICAS
    # =========================================================================
    if tipo_archivo == 'BM':
        columnas_criticas = ['timestamp', 'answers']
        columnas_unicas = ['timestamp', 'answers', 'custIdentNum']
    else:  # BV
        columnas_criticas = ['Date Submitted']
        columnas_unicas = ['Date Submitted']

    for col in columnas_criticas:
        if col not in df.columns:
            validacion['columnas_criticas_faltantes'].append(col)
            validacion['criticos'].append({
                'tipo': 'columna_faltante',
                'columna': col,
                'mensaje': f"Columna cr√≠tica '{col}' no existe"
            })

    # =========================================================================
    # 2. AN√ÅLISIS DE DUPLICADOS INTELIGENTE
    # =========================================================================
    # Duplicados por ID (puede ser falso positivo)
    if tipo_archivo == 'BM' and 'id' in df.columns:
        validacion['duplicados_por_id'] = int(df['id'].duplicated().sum())
    elif tipo_archivo == 'BV' and 'Date Submitted' in df.columns:
        validacion['duplicados_por_id'] = int(df['Date Submitted'].duplicated().sum())

    # Duplicados REALES (comparando columnas importantes)
    columnas_disponibles = [col for col in columnas_unicas if col in df.columns]
    if columnas_disponibles:
        validacion['duplicados_reales'] = int(df[columnas_disponibles].duplicated().sum())

    # Evaluar severidad de duplicados
    if validacion['duplicados_reales'] > 0:
        if validacion['duplicados_reales'] > len(df) * 0.5:  # >50% duplicados
            validacion['criticos'].append({
                'tipo': 'duplicados_masivos',
                'cantidad': validacion['duplicados_reales'],
                'mensaje': f"M√°s del 50% de registros est√°n completamente duplicados ({validacion['duplicados_reales']:,})"
            })
        else:
            validacion['advertencias'].append({
                'tipo': 'duplicados',
                'cantidad': validacion['duplicados_reales'],
                'mensaje': f"{validacion['duplicados_reales']:,} registros completamente duplicados detectados"
            })

    # Si hay duplicados por ID pero NO duplicados reales, es solo advertencia
    if validacion['duplicados_por_id'] > 0 and validacion['duplicados_reales'] == 0:
        validacion['advertencias'].append({
            'tipo': 'ids_duplicados',
            'cantidad': validacion['duplicados_por_id'],
            'mensaje': f"{validacion['duplicados_por_id']} IDs duplicados pero respuestas √∫nicas (no bloquea)"
        })

    # =========================================================================
    # 3. AN√ÅLISIS DE VALORES NULOS
    # =========================================================================
    for col in df.columns:
        nulos = df[col].isna().sum()
        if nulos > 0:
            porcentaje = round(nulos / len(df) * 100, 2)
            validacion['valores_nulos_por_columna'][col] = {
                'cantidad': int(nulos),
                'porcentaje': porcentaje
            }

            # Evaluar severidad seg√∫n columna y porcentaje
            if col in columnas_criticas and porcentaje > 50:
                validacion['criticos'].append({
                    'tipo': 'nulos_criticos',
                    'columna': col,
                    'cantidad': nulos,
                    'porcentaje': porcentaje,
                    'mensaje': f"Columna cr√≠tica '{col}' tiene {porcentaje}% de valores nulos"
                })
            elif col in columnas_criticas and porcentaje > 10:
                validacion['advertencias'].append({
                    'tipo': 'nulos_moderados',
                    'columna': col,
                    'cantidad': nulos,
                    'porcentaje': porcentaje,
                    'mensaje': f"Columna '{col}' tiene {porcentaje}% de valores nulos"
                })

    # =========================================================================
    # 4. VALIDACI√ìN FILA POR FILA (solo primeras 1000)
    # =========================================================================
    filas_a_validar = min(1000, len(df))

    for idx in range(filas_a_validar):
        fila = df.iloc[idx]
        tiene_critico = False
        tiene_advertencia = False
        tiene_informativo = False

        if tipo_archivo == 'BM':
            # Validar timestamp
            if 'timestamp' in df.columns and pd.isna(fila['timestamp']):
                tiene_advertencia = True
                if len(validacion['advertencias']) < 100:
                    validacion['advertencias'].append({
                        'tipo': 'timestamp_nulo',
                        'fila': idx + 2,
                        'columna': 'timestamp',
                        'mensaje': 'Timestamp nulo'
                    })

            # Validar answers
            if 'answers' in df.columns:
                valor_answers = fila['answers']

                if pd.isna(valor_answers) or str(valor_answers).strip() == '':
                    tiene_critico = True
                    if len(validacion['criticos']) < 100:
                        validacion['criticos'].append({
                            'tipo': 'answers_vacio',
                            'fila': idx + 2,
                            'columna': 'answers',
                            'mensaje': 'Campo answers vac√≠o o nulo'
                        })

                elif isinstance(valor_answers, str):
                    # Detectar encoding corrupto (INFORMATIVO - se corrige en limpieza)
                    if '√É' in valor_answers or '√Ç' in valor_answers:
                        tiene_informativo = True
                        validacion['errores_encoding'] += 1
                        validacion['tiene_encoding_corrupto'] = True

        else:  # BV
            # Validar Date Submitted
            if 'Date Submitted' in df.columns and pd.isna(fila['Date Submitted']):
                tiene_critico = True
                if len(validacion['criticos']) < 100:
                    validacion['criticos'].append({
                        'tipo': 'fecha_nula',
                        'fila': idx + 2,
                        'columna': 'Date Submitted',
                        'mensaje': 'Fecha de env√≠o nula'
                    })

        # Contabilizar por severidad
        if tiene_critico:
            validacion['filas_con_criticos'] += 1
        elif tiene_advertencia:
            validacion['filas_con_advertencias'] += 1
        elif tiene_informativo:
            validacion['filas_con_informativos'] += 1
        else:
            validacion['filas_validas'] += 1

    # =========================================================================
    # 5. AGREGAR RESUMEN DE ENCODING CORRUPTO
    # =========================================================================
    if validacion['errores_encoding'] > 0:
        validacion['informativos'].append({
            'tipo': 'encoding_corrupto',
            'cantidad': validacion['errores_encoding'],
            'mensaje': f"Encoding UTF-8 corrupto en {validacion['errores_encoding']} registros (se corregir√° en 2_limpieza.py)",
            'auto_corregible': True
        })

    # =========================================================================
    # 6. CALCULAR TASA DE CALIDAD REAL
    # =========================================================================
    # No contar informativos (auto-corregibles) como errores
    filas_con_errores_reales = validacion['filas_con_criticos'] + validacion['filas_con_advertencias']
    filas_procesables = validacion['total_filas'] - validacion['filas_con_criticos']

    if validacion['total_filas'] > 0:
        tasa_calidad = (filas_procesables / validacion['total_filas']) * 100
        validacion['tasa_calidad'] = round(tasa_calidad, 2)
    else:
        validacion['tasa_calidad'] = 0.0

    # =========================================================================
    # 7. DETERMINAR ESTADO FINAL
    # =========================================================================
    if len(validacion['criticos']) > 0 or validacion['tasa_calidad'] < 50:
        validacion['estado'] = 'CRITICO'
        validacion['puede_continuar'] = False
    elif validacion['tasa_calidad'] >= 95:
        validacion['estado'] = 'EXCELENTE'
        validacion['puede_continuar'] = True
    elif validacion['tasa_calidad'] >= 80:
        validacion['estado'] = 'BUENO'
        validacion['puede_continuar'] = True
    else:
        validacion['estado'] = 'ACEPTABLE'
        validacion['puede_continuar'] = True

    # =========================================================================
    # 8. GENERAR ARCHIVO DE VALIDACI√ìN SIMPLIFICADO
    # =========================================================================
    archivo_validacion = ruta_archivo_salida.replace('.xlsx', '.validation')
    try:
        with open(archivo_validacion, 'w', encoding='utf-8') as f:
            f.write("="*80 + "\n")
            f.write("VALIDACI√ìN DE DATOS\n")
            f.write("="*80 + "\n\n")

            # RESUMEN COMPACTO
            f.write(f"üìÑ Archivo: {os.path.basename(ruta_archivo_salida)}\n")
            f.write(f"üìÖ Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")

            # M√âTRICAS PRINCIPALES
            entrada_total = validacion['total_filas']
            salida_procesable = validacion['total_filas'] - validacion['filas_con_criticos']

            f.write(f"ENTRADA: {entrada_total:,} registros\n")
            f.write(f"SALIDA PROCESABLE: {salida_procesable:,} registros\n")
            f.write(f"CALIDAD: {validacion['tasa_calidad']:.1f}%")

            if validacion['estado'] == 'EXCELENTE':
                f.write(" ‚úÖ EXCELENTE\n\n")
            elif validacion['estado'] == 'BUENO':
                f.write(" ‚úÖ BUENO\n\n")
            elif validacion['estado'] == 'ACEPTABLE':
                f.write(" ‚ö†Ô∏è  ACEPTABLE\n\n")
            else:
                f.write(" ‚ùå CR√çTICO\n\n")

            if validacion['puede_continuar']:
                f.write("RESULTADO: ‚úÖ Listo para procesamiento\n")
            else:
                f.write("RESULTADO: ‚ùå Revisar antes de continuar\n")

            f.write("\n" + "-"*80 + "\n\n")

            # ERRORES CR√çTICOS (con n√∫meros de fila)
            f.write("üî¥ ERRORES CR√çTICOS (bloquean procesamiento): {}\n".format(len(validacion['criticos'])))
            if validacion['criticos']:
                f.write("-"*80 + "\n")
                for i, error in enumerate(validacion['criticos'][:20], 1):
                    fila = error.get('fila', '?')
                    columna = error.get('columna', '?')
                    mensaje = error.get('mensaje', 'Error')
                    tipo = error.get('tipo', '')

                    # Formato: Fila 123 | columna | mensaje
                    if 'fila' in error:
                        f.write(f"{i}. Fila {fila} | {columna} | {mensaje}\n")
                    else:
                        f.write(f"{i}. {mensaje}\n")

                if len(validacion['criticos']) > 20:
                    f.write(f"\n... y {len(validacion['criticos']) - 20} errores m√°s (revisar archivo Excel)\n")
            else:
                f.write("   ‚úÖ Ninguno\n")
            f.write("\n")

            # ADVERTENCIAS (con n√∫meros de fila)
            f.write("‚ö†Ô∏è  ADVERTENCIAS (revisar pero no bloquean): {}\n".format(len(validacion['advertencias'])))
            if validacion['advertencias']:
                f.write("-"*80 + "\n")
                for i, error in enumerate(validacion['advertencias'][:20], 1):
                    fila = error.get('fila', '?')
                    columna = error.get('columna', '?')
                    mensaje = error.get('mensaje', 'Advertencia')

                    if 'fila' in error:
                        f.write(f"{i}. Fila {fila} | {columna} | {mensaje}\n")
                    else:
                        f.write(f"{i}. {mensaje}\n")

                if len(validacion['advertencias']) > 20:
                    f.write(f"\n... y {len(validacion['advertencias']) - 20} advertencias m√°s\n")
            else:
                f.write("   ‚úÖ Ninguna\n")
            f.write("\n")

            # INFORMATIVOS (compacto)
            if validacion['informativos']:
                f.write("‚ÑπÔ∏è  INFORMATIVOS (se corrigen en 2_limpieza.py):\n")
                f.write("-"*80 + "\n")
                for info in validacion['informativos']:
                    f.write(f"‚Ä¢ {info.get('mensaje', 'Info')}\n")
                f.write("\n")

            # DUPLICADOS (simplificado)
            if validacion['duplicados_por_id'] > 0 or validacion['duplicados_reales'] > 0:
                f.write("üîÑ DUPLICADOS:\n")
                f.write("-"*80 + "\n")
                f.write(f"Duplicados por ID: {validacion['duplicados_por_id']:,}\n")
                f.write(f"Duplicados reales: {validacion['duplicados_reales']:,}\n\n")

                if validacion['duplicados_reales'] == 0 and validacion['duplicados_por_id'] > 0:
                    f.write("‚úÖ OK - IDs repetidos pero respuestas √∫nicas (no bloquea)\n\n")
                elif validacion['duplicados_reales'] > 0:
                    f.write(f"‚ö†Ô∏è  {validacion['duplicados_reales']:,} registros completamente duplicados\n\n")

            # VALORES NULOS (solo columnas cr√≠ticas con >10% nulos)
            nulos_criticos = {col: info for col, info in validacion['valores_nulos_por_columna'].items()
                            if col in columnas_criticas and info['porcentaje'] > 10}

            if nulos_criticos:
                f.write("‚ö†Ô∏è  VALORES NULOS EN COLUMNAS CR√çTICAS:\n")
                f.write("-"*80 + "\n")
                for col, info in sorted(nulos_criticos.items(), key=lambda x: x[1]['porcentaje'], reverse=True):
                    f.write(f"‚Ä¢ {col}: {info['cantidad']:,} nulos ({info['porcentaje']:.1f}%)\n")
                f.write("\n")

            # PR√ìXIMOS PASOS
            f.write("-"*80 + "\n")
            if validacion['puede_continuar']:
                f.write("‚úÖ PR√ìXIMOS PASOS:\n")
                f.write("   python 2_limpieza.py      ‚Üí Limpiar datos\n")
                f.write("   python 3_insercion.py     ‚Üí Insertar en PostgreSQL\n")
                f.write("   python 4_visualizacion.py ‚Üí Generar dashboard\n")
            else:
                f.write("‚ùå ACCI√ìN REQUERIDA:\n")
                f.write("   1. Corregir errores cr√≠ticos en archivo Excel original\n")
                f.write("   2. Ejecutar: python 1_extractor.py --force\n")

            f.write("\n" + "="*80 + "\n")

        logger.info(f"üìã Reporte de validaci√≥n generado: {os.path.basename(archivo_validacion)}")

    except Exception as e:
        logger.error(f"Error generando reporte de validaci√≥n: {e}")

    return validacion


def extraer_datos(ruta_archivo, max_registros=None, directorio_salida=DIRECTORIO_SALIDA):
    """
    Extrae datos de un archivo Excel y los guarda en formato Excel para procesamiento

    Args:
        ruta_archivo (str): Ruta completa al archivo Excel de entrada
        max_registros (int): N√∫mero m√°ximo de registros a extraer (None = todos)
        directorio_salida (str): Carpeta donde guardar los datos extra√≠dos

    Returns:
        tuple: (ruta_archivo_salida, cantidad_registros, info_extraccion) o (None, 0, {}) si hay error
    """
    info_extraccion = {
        'archivo_original': os.path.basename(ruta_archivo),
        'exitoso': False,
        'advertencias': []
    }

    print(f"\nüìÇ Procesando: {ruta_archivo}")
    logger.info(f"Iniciando extracci√≥n: {ruta_archivo}")

    try:
        # Leer archivo Excel completo
        print("   ‚è≥ Leyendo archivo Excel...")
        df_completo = pd.read_excel(ruta_archivo)
        total_filas = len(df_completo)

        info_extraccion['total_original'] = total_filas
        info_extraccion['total_columnas'] = len(df_completo.columns)

        print(f"   üìä Total de registros en archivo: {total_filas:,}")
        print(f"   üìã Columnas disponibles: {len(df_completo.columns)}")

        # Decidir cu√°ntos registros extraer
        if max_registros is None or max_registros == 0:
            print(f"   ‚ÑπÔ∏è  Extrayendo TODOS los registros ({total_filas:,})")
            datos_extraidos = df_completo
        elif total_filas <= max_registros:
            print(f"   ‚ÑπÔ∏è  Archivo tiene {total_filas:,} registros, extrayendo todos")
            datos_extraidos = df_completo
        else:
            print(f"   ‚ö†Ô∏è  Limitando extracci√≥n a {max_registros:,} registros")
            datos_extraidos = df_completo.head(max_registros)
            info_extraccion['advertencias'].append(f"Archivo limitado a {max_registros:,} registros")

        info_extraccion['registros_extraidos'] = len(datos_extraidos)

        # Crear directorio de salida si no existe
        Path(directorio_salida).mkdir(exist_ok=True)

        # Generar nombre de archivo de salida
        nombre_base = Path(ruta_archivo).stem
        archivo_salida = Path(directorio_salida) / f"{nombre_base}_extracted_{len(datos_extraidos)}.xlsx"

        # Guardar datos extra√≠dos
        print(f"   üíæ Guardando datos extra√≠dos...")
        datos_extraidos.to_excel(archivo_salida, index=False)

        print(f"   ‚úÖ Datos guardados: {archivo_salida}")
        print(f"   üìà Registros extra√≠dos: {len(datos_extraidos):,}")

        # Identificar y mostrar columnas importantes
        columnas_importantes = []
        for col in datos_extraidos.columns:
            if any(palabra in col.lower() for palabra in ['answer', 'nps', 'score', 'timestamp', 'date', 'recomien']):
                columnas_importantes.append(col)

        info_extraccion['columnas_importantes'] = columnas_importantes

        if columnas_importantes:
            print(f"\n   üìä COLUMNAS CLAVE DETECTADAS:")
            for col in columnas_importantes[:5]:  # Mostrar m√°ximo 5
                print(f"      ‚Ä¢ {col}")

        # Detectar problemas potenciales
        if 'answers' in datos_extraidos.columns:
            # Verificar si hay JSON malformado (espec√≠fico de BM)
            muestra_answers = datos_extraidos['answers'].dropna().head(3)
            for answer in muestra_answers:
                if isinstance(answer, str) and '√É' in answer:
                    info_extraccion['advertencias'].append("Encoding UTF-8 malformado detectado en columna 'answers'")
                    break

        info_extraccion['exitoso'] = True
        info_extraccion['tipo'] = 'BM' if '_bm_' in nombre_base.lower() or 'bm_' in nombre_base.lower() else 'BV'

        # Validar datos detalladamente
        print(f"\n   üîç Validando calidad de datos...")
        validacion = validar_datos_detallado(datos_extraidos, str(archivo_salida), info_extraccion['tipo'])

        # Agregar resultados de validaci√≥n al info
        info_extraccion['validacion'] = validacion

        # RESUMEN COMPACTO EN CONSOLA
        entrada = info_extraccion['total_original']
        salida = info_extraccion['registros_extraidos']
        calidad = validacion['tasa_calidad']
        estado = validacion['estado']

        # Emoji seg√∫n calidad
        if estado == 'EXCELENTE':
            emoji_calidad = '‚úÖ'
        elif estado == 'BUENO':
            emoji_calidad = '‚úÖ'
        elif estado == 'ACEPTABLE':
            emoji_calidad = '‚ö†Ô∏è '
        else:
            emoji_calidad = '‚ùå'

        print(f"\n   {'='*70}")
        print(f"   ENTRADA: {entrada:,} | SALIDA: {salida:,} | CALIDAD: {calidad:.1f}% {emoji_calidad} {estado}")

        # Mostrar solo si hay problemas
        criticos = len(validacion['criticos'])
        advertencias = len(validacion['advertencias'])

        if criticos > 0 or advertencias > 0:
            print(f"   {criticos} cr√≠ticos, {advertencias} advertencias ‚Üí Ver: {os.path.basename(str(archivo_salida).replace('.xlsx', '.validation'))}")

        if validacion['duplicados_reales'] > 0:
            print(f"   ‚ö†Ô∏è  {validacion['duplicados_reales']:,} duplicados reales detectados")
            info_extraccion['advertencias'].append(f"{validacion['duplicados_reales']} registros duplicados detectados")

        print(f"   {'='*70}")

        # Generar log individual
        generar_log_individual(str(archivo_salida), info_extraccion)

        logger.info(f"Extracci√≥n exitosa: {archivo_salida} ({len(datos_extraidos):,} registros)")

        return archivo_salida, len(datos_extraidos), info_extraccion

    except Exception as e:
        print(f"   ‚ùå Error procesando {ruta_archivo}: {str(e)}")
        logger.error(f"Error en extracci√≥n de {ruta_archivo}: {e}")
        info_extraccion['advertencias'].append(f"Error: {str(e)}")
        return None, 0, info_extraccion


def buscar_archivos_datos(directorio_base=DIRECTORIO_ENTRADA, archivo_especifico=None):
    """
    Escanea el directorio base y encuentra todos los archivos Excel organizados por mes

    Args:
        directorio_base (str): Ruta al directorio que contiene las carpetas de meses
        archivo_especifico (str): Nombre de archivo espec√≠fico a procesar (opcional)

    Returns:
        dict: Diccionario con estructura {mes: {'bm': ruta, 'bv': ruta}}
    """
    archivos_por_mes = {}

    if not os.path.exists(directorio_base):
        logger.error(f"Directorio '{directorio_base}' no encontrado")
        print(f"‚ùå Directorio '{directorio_base}' no encontrado")
        print(f"   üí° Crea el directorio y organiza los archivos por mes")
        return archivos_por_mes

    print(f"\nüîç Escaneando directorio: {directorio_base}")
    logger.info(f"Escaneando directorio: {directorio_base}")

    # Escanear subdirectorios (cada uno representa un mes)
    for carpeta_mes in os.listdir(directorio_base):
        ruta_mes = os.path.join(directorio_base, carpeta_mes)

        if os.path.isdir(ruta_mes):
            print(f"\nüìÅ Escaneando mes: {carpeta_mes}")
            archivos_por_mes[carpeta_mes] = {}

            # Buscar archivos Excel en la carpeta del mes
            for archivo in os.listdir(ruta_mes):
                if archivo.endswith(('.xlsx', '.xls')):
                    # Si se especific√≥ un archivo, solo procesar ese
                    if archivo_especifico and archivo != archivo_especifico:
                        continue

                    ruta_archivo = os.path.join(ruta_mes, archivo)

                    # Identificar tipo de archivo (BM o BV)
                    if '_BM_' in archivo or 'BM_' in archivo or '_bm_' in archivo.lower():
                        archivos_por_mes[carpeta_mes]['bm'] = ruta_archivo
                        print(f"   ‚úÖ Banco M√≥vil (BM): {archivo}")
                        logger.info(f"Encontrado BM: {ruta_archivo}")
                    elif '_BV_' in archivo or 'BV_' in archivo or '_bv_' in archivo.lower():
                        archivos_por_mes[carpeta_mes]['bv'] = ruta_archivo
                        print(f"   ‚úÖ Banco Virtual (BV): {archivo}")
                        logger.info(f"Encontrado BV: {ruta_archivo}")
                    else:
                        print(f"   ‚ö†Ô∏è  Archivo no identificado: {archivo}")
                        logger.warning(f"Archivo sin tipo identificado: {archivo}")

    return archivos_por_mes


# ======================================================================================
# FUNCI√ìN PRINCIPAL
# ======================================================================================

def main():
    """
    Funci√≥n principal que coordina la extracci√≥n de datos de todos los meses
    """
    # Parsear argumentos de l√≠nea de comandos
    parser = argparse.ArgumentParser(
        description='Extractor de Datos NPS - Pipeline de Producci√≥n',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Ejemplos de uso:
  python 1_extractor.py                              # Procesa archivos nuevos
  python 1_extractor.py --full                       # Procesa archivos completos
  python 1_extractor.py --limit 5000                 # Limita a 5000 registros
  python 1_extractor.py --file "Agosto_BM_2025.xlsx" # Solo un archivo
  python 1_extractor.py --force                      # Fuerza reprocesamiento
        """
    )

    parser.add_argument(
        '--full',
        action='store_true',
        help='Procesar archivos completos sin l√≠mite de registros'
    )

    parser.add_argument(
        '--limit',
        type=int,
        default=None,
        help='N√∫mero m√°ximo de registros a extraer por archivo'
    )

    parser.add_argument(
        '--file',
        type=str,
        default=None,
        help='Nombre espec√≠fico de archivo a procesar (ej: Agosto_BM_2025.xlsx)'
    )

    parser.add_argument(
        '--force',
        action='store_true',
        help='Forzar reprocesamiento de archivos ya procesados'
    )

    args = parser.parse_args()

    # Determinar l√≠mite de registros
    if args.full:
        max_registros = None
        modo_extraccion = "COMPLETO (sin l√≠mite)"
    elif args.limit:
        max_registros = args.limit
        modo_extraccion = f"LIMITADO a {max_registros:,} registros"
    else:
        max_registros = None
        modo_extraccion = "COMPLETO (sin l√≠mite)"

    print("\n" + "="*80)
    print("üöÄ EXTRACTOR DE DATOS NPS - PIPELINE DE PRODUCCI√ìN")
    print("="*80)
    print(f"\n‚öôÔ∏è  MODO: {modo_extraccion}")

    if args.file:
        print(f"üìÑ Procesando solo: {args.file}")

    if args.force:
        print(f"‚ö†Ô∏è  MODO FORZADO: Se reprocesar√°n todos los archivos")

    logger.info(f"Iniciando extracci√≥n - Modo: {modo_extraccion}")

    # Cargar archivos ya procesados
    archivos_procesados = cargar_archivos_procesados()
    print(f"\nüìù Archivos previamente procesados: {len(archivos_procesados)}")
    logger.info(f"Archivos en tracking: {len(archivos_procesados)}")

    # Buscar archivos organizados por mes
    archivos_por_mes = buscar_archivos_datos(DIRECTORIO_ENTRADA, args.file)

    if not archivos_por_mes:
        print(f"\n‚ùå No se encontraron archivos en '{DIRECTORIO_ENTRADA}/'")
        print(f"\nüí° ESTRUCTURA ESPERADA:")
        print(f"   {DIRECTORIO_ENTRADA}/")
        print(f"   ‚îú‚îÄ‚îÄ Agosto/")
        print(f"   ‚îÇ   ‚îú‚îÄ‚îÄ Agosto_BM_2025.xlsx")
        print(f"   ‚îÇ   ‚îî‚îÄ‚îÄ Agosto_BV_2025.xlsx")
        print(f"   ‚îú‚îÄ‚îÄ Septiembre/")
        print(f"   ‚îÇ   ‚îú‚îÄ‚îÄ Septiembre_BM_2025.xlsx")
        print(f"   ‚îÇ   ‚îî‚îÄ‚îÄ Septiembre_BV_2025.xlsx")
        logger.warning("No se encontraron archivos para procesar")
        return

    resultados = []
    omitidos = []
    errores = []

    # Procesar cada mes encontrado
    for mes, archivos in archivos_por_mes.items():
        print(f"\n" + "="*80)
        print(f"üìÖ PROCESANDO MES: {mes}")
        print("="*80)

        # Procesar archivo de Banco M√≥vil (BM)
        if 'bm' in archivos:
            ruta_bm = archivos['bm']

            # Verificar si ya fue procesado
            if not args.force and archivo_ya_procesado(ruta_bm, archivos_procesados):
                print(f"\nüè¶ Banco M√≥vil - {mes}")
                print(f"   ‚è≠Ô∏è  Omitiendo: {os.path.basename(ruta_bm)} (ya procesado)")
                logger.info(f"Omitido (ya procesado): {ruta_bm}")
                omitidos.append((mes, 'BM', ruta_bm))
            else:
                print(f"\nüè¶ Banco M√≥vil - {mes}")
                archivo_salida, cantidad, info = extraer_datos(ruta_bm, max_registros=max_registros)

                if archivo_salida:
                    resultados.append((mes, 'BM', ruta_bm, archivo_salida, cantidad))

                    # Registrar en tracking
                    hash_archivo = obtener_hash_archivo(ruta_bm)
                    if hash_archivo:
                        registrar_archivo_procesado(ruta_bm, hash_archivo)
                else:
                    errores.append((mes, 'BM', ruta_bm))

        # Procesar archivo de Banco Virtual (BV)
        if 'bv' in archivos:
            ruta_bv = archivos['bv']

            # Verificar si ya fue procesado
            if not args.force and archivo_ya_procesado(ruta_bv, archivos_procesados):
                print(f"\nüíª Banco Virtual - {mes}")
                print(f"   ‚è≠Ô∏è  Omitiendo: {os.path.basename(ruta_bv)} (ya procesado)")
                logger.info(f"Omitido (ya procesado): {ruta_bv}")
                omitidos.append((mes, 'BV', ruta_bv))
            else:
                print(f"\nüíª Banco Virtual - {mes}")
                archivo_salida, cantidad, info = extraer_datos(ruta_bv, max_registros=max_registros)

                if archivo_salida:
                    resultados.append((mes, 'BV', ruta_bv, archivo_salida, cantidad))

                    # Registrar en tracking
                    hash_archivo = obtener_hash_archivo(ruta_bv)
                    if hash_archivo:
                        registrar_archivo_procesado(ruta_bv, hash_archivo)
                else:
                    errores.append((mes, 'BV', ruta_bv))

    # Mostrar resumen final
    print("\n" + "="*80)
    print("üìä RESUMEN DE EXTRACCI√ìN")
    print("="*80)

    if resultados:
        total_registros = 0
        print(f"\n‚úÖ ARCHIVOS PROCESADOS EXITOSAMENTE ({len(resultados)}):")

        for mes, tipo, original, archivo_salida, cantidad in resultados:
            print(f"\n   {tipo} - {mes}:")
            print(f"      Original: {os.path.basename(original)}")
            print(f"      Extra√≠do: {os.path.basename(archivo_salida)}")
            print(f"      Registros: {cantidad:,}")
            total_registros += cantidad

        print(f"\nüìà TOTAL: {len(resultados)} archivos procesados, {total_registros:,} registros extra√≠dos")
        logger.info(f"Extracci√≥n completada: {len(resultados)} archivos, {total_registros:,} registros")

    if omitidos:
        print(f"\n‚è≠Ô∏è  ARCHIVOS OMITIDOS (ya procesados): {len(omitidos)}")
        for mes, tipo, original in omitidos:
            print(f"   ‚Ä¢ {tipo} - {mes}: {os.path.basename(original)}")
        logger.info(f"Archivos omitidos: {len(omitidos)}")

    if errores:
        print(f"\n‚ùå ARCHIVOS CON ERRORES: {len(errores)}")
        for mes, tipo, original in errores:
            print(f"   ‚Ä¢ {tipo} - {mes}: {os.path.basename(original)}")
        logger.error(f"Archivos con errores: {len(errores)}")

    if not resultados and not omitidos:
        print("\n‚ùå No se procesaron archivos")
        print(f"\nüí° Verifica:")
        print(f"   ‚Ä¢ La estructura de carpetas en '{DIRECTORIO_ENTRADA}/'")
        print(f"   ‚Ä¢ Que los archivos tengan '_BM_' o '_BV_' en el nombre")
        print(f"   ‚Ä¢ Que los archivos sean formato Excel (.xlsx o .xls)")
        logger.warning("No se procesaron archivos")
    else:
        print(f"\nüéØ PR√ìXIMOS PASOS:")
        print(f"   1. Revisar logs individuales (.txt) en '{DIRECTORIO_SALIDA}/'")
        print(f"   2. python 2_limpieza.py      # Limpiar y transformar datos")
        print(f"   3. python 3_insercion.py     # Insertar en PostgreSQL")
        print(f"   4. python 4_visualizacion.py # Generar dashboard")

        print(f"\nüìù Archivo de tracking actualizado: {ARCHIVO_TRACKING}")
        print(f"   Total de archivos registrados: {len(cargar_archivos_procesados())}")


if __name__ == "__main__":
    main()
